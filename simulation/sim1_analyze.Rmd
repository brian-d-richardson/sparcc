---
title: "Sparcc Estimation Example 1, Program 8"
author: "Brian Richardson"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output:
  html_document:
    code_folding: hide
---

```{r message = F, warning = F}

rm(list = ls())
library(dplyr)
library(tidyverse)
library(ggplot2)
library(ggh4x)
library(kableExtra)

```

## Simulation Setup

The goal of this program is to evaluate the performance of the semiparametric efficient estimator $\widehat{\pmb{\beta}}$ using simulations in the following setting:

* $X \sim \textrm{gamma}(\theta_1, \theta_2)$,
* $C \sim \textrm{gamma}(\alpha_1, \alpha_2)$,
* $Y|X \sim N(\beta_0 + \beta_1X, 1)$.

In Example 1, we treat the nuisance distributions for $X$ and $C$ as known, but all programming is done to allow any posited/estimated distributions. The assumption that $Y|X$ follows a normal distribution allows us to use Gaussian quadrature to efficiently solve integrals with respect to $y$. 

In these simulations, we vary the censoring probability $q=\textrm{P}(X>C)$ between $q=0.4$ (moderate censoring) and $q=0.8$ (heavy censoring.

We also vary which nuisance distributions are correct. In all settings, the MLE and semiparametric estimator assume $X$ and $C$ follow exponential distributions (which are special cases of the gamma distribution with shape parameter equal to 1). To simulate the scenarios where $X$ and/or $C$ (or neither) are misspecified, we do the following:

* $X$ and $C$ correct: $X \sim \textrm{gamma}(1, 1/4) = \textrm{exponential}(1/4)$ and $C \sim \textrm{gamma}(1, \alpha_2) = \textrm{exponential}(\alpha_2)$, where $\alpha_2$ is chosen such that $\textrm{P}(X>C) = q$,
* $X$ incorrect, $C$ correct: $X \sim \textrm{gamma}(4, \theta_2)$ and $C \sim \textrm{gamma}(1, \alpha_2)$, where $\theta_2, \alpha_2$ are chosen such that $\textrm{E}(X) = 1/4$ and $\textrm{P}(X>C) = q$,
* $X$ correct, $C$ incorrect: $X \sim \textrm{gamma}(1, 1/4)$ and $C \sim \textrm{gamma}(4, \alpha_2)$, where $\alpha_2$ is chosen such that $\textrm{P}(X>C) = q$,
* $X$ and $C$ incorrect: $X \sim \textrm{gamma}(4, \theta_2)$ and $C \sim \textrm{gamma}(4, \alpha_2)$, where $\theta_2, \alpha_2$ are chosen such that $\textrm{E}(X) = 1/4$ and $\textrm{P}(X>C) = q$.

In all settings, $\textrm{E}(X) = 1/4$ and $\textrm{P}(X>C) = q$.

For each simulation, we generate a data set of size $n=10,000$ then compute the complete case estimator $\widehat{\pmb{\beta}}_{cc}$, the oracle estimator $\widehat{\pmb{\beta}}_{or}$, the parametric MLE $\widehat{\pmb{\beta}}_{ml}$, and the semiparametric efficient estimator $\widehat{\pmb{\beta}}$.

## Computing the Semiparametric Estimator

Our current approach for computing $S_{eff}$ is as follows.

* Approximate the integral equation in Proposition 3 using quadrature rules for $X$ and $C$. In particular, assign point mass $r_j = \eta_1(x_j) / \sum_{k=1}^m \eta_1(x_k)$ to each of $m_x$ nodes $x_j$ in the support of $X$, and point mass $t_j = \eta_2(c_j) / \sum_{k=1}^m \eta_2(c_k)$ to each of $m_c$ nodes $c_j$ in the support of $C$.

* Replace all integrals with respect to $x$ or $c$ with finite sums, using the above quadrature rules. For example, in $\textrm{LHS}_j$, we let $\textrm{E}\{\textrm{I}(x_j\leq C\} = \int_{x_j}^\infty\eta_2(c)\approx\sum_{k=1}^{m_c}t_k\textrm{I}(c_k\geq x_j)$.

* Evaluate integrals with respect to $y$ using Gaussian quadrature using $m_y$ nodes. Let $w_h, \phi_h$ be the nodes and weights for the integral approximation $\int g(u)\exp(-u^2)du \approx \sum_{h=1}^{m_y}g(w_h)\phi_h$. Then all integrals with $f_{Y|X}(y,x_i,\pmb{\beta})$ or $f_{Y|X}(y,x_i,\pmb{\beta})f_{Y|X}(y,x_j,\pmb{\beta})$ in the integrand can be approximated using:

$$
\int g(y)f_{Y|X}(y,x_i,\pmb{\beta})dy \\
= \int g(y)(2\pi)^{-1/2}\exp\left\{-\frac{1}{2}(y-\mu_i)^2\right\}dy \\ 
= \pi^{-1/2}\int g(\sqrt{2}u + \mu_i)\exp(-u^2)du \\
\approx \pi^{-1/2}\sum_{h=1}^{m_y}g(\sqrt{2}w_h + \mu_i)\phi_h,
$$

and

$$
\int g(y)f_{Y|X}(y,x_i,\pmb{\beta})f_{Y|X}(y,x_j,\pmb{\beta})dy \\
= \int g(y)(2\pi)^{-1}\exp\left\{-\frac{1}{4}(\mu_i-\mu_j)^2\right\}\exp\{-(y-\bar{\mu}_{ij})^2dy \\ 
= (2\pi)^{-1}\exp\left\{-\frac{1}{4}(\mu_i-\mu_j)^2\right\}\int g(u + \bar{\mu}_{ij})\exp(-u^2)du \\
\approx (2\pi)^{-1}\exp\left\{-\frac{1}{4}(\mu_i-\mu_j)^2\right\}\sum_{h=1}^{m_y}g(w_h + \bar{\mu}_{ij})\phi_h.
$$

* Solve for $\eta_1(x_j)\pmb{a}(x_j, \pmb{\beta})$ at each node $x_j$. This involves solving a symmetric system of linear equations, which is done stably and quickly using the Cholesky decomposition. Then use linear interpolation to evaluate $\eta_1(x)\pmb{a}(x,\pmb{\beta})$ at any observed $x$ value that is not a node in the quadrature rule for $X$.

* Use the obtained $\pmb{a}(x,\pmb{\beta})$ values and the quadrature rules for $X$ and $C$ to approximate the efficient score vector $S_{eff}$, evaluated at the observed data $(y_i,w_i,\delta_i)$ for each observation $i$.

* In a previous set of simulations, we showed that $m_y=2$ and $m_c=15$ are sufficiently fine grids for approximating $S_{eff}$, and that $m_x$ may need to be as large as $100$. We will use these values for now.

## Simulation Results

We first look at which simulation settings lead to errors. For 15/1000 simulations, the semiparametric estiamator ("sp") with $X$ correct and $C$ incorrect leads to an error in the root search.

```{r}

# true (beta, log s2)
B <- c(1, 2, log(0.81))

# load simulation results from each of 10 clusters
sim.out.list <- lapply(
  X = 0:9,
  FUN = function(clust) {
    cbind(clust,
          read.csv(paste0("sim_data/sim1/sd",
                          clust, ".csv")))
  })


# combine simulation results into 1 data frame
sim.out <- bind_rows(sim.out.list)

# make long data frame
sim.out.long <- sim.out %>% 
  pivot_longer(cols = starts_with("B"),
               names_to = "method.param",
               values_to = "estimate") %>% 
    mutate(method = factor(substr(method.param, 2, 3),
                           levels = c("or", "cc", "ml", "sp")),
           param = factor(substr(method.param, 4, 4)),
           B.true = B[param])
  
```

```{r message = F}

# check for simulations with errors
sim.out.long %>% 
  filter(param == 1) %>% 
  group_by(q, n, x.shape, c.shape, method) %>% 
  summarize(prop.error = mean(is.na(estimate))) %>% 
  filter(prop.error > 0) %>% 
  kable(digits = 3) %>%
  kable_styling("striped")

```

```{r}

# extract simulation parameters
q <- unique(sim.out$q)
n <- unique(sim.out$n)
x.shape <- unique(sim.out$x.shape)
c.shape <- unique(sim.out$c.shape)
mx <- unique(sim.out$mx)
mc <- unique(sim.out$mc)
my <- unique(sim.out$my)
n.rep <- nrow(sim.out) /
  n_distinct(dplyr::select(sim.out, q, n, x.shape, c.shape, mx, mc, my))

# make labels for plots
method.labs <- c("Oracle",
                 "Complete Case",
                 "Parametric MLE",
                 "Semiparametric")

names(method.labs) <- c("or", "cc", "ml", "sp")

q.labs <- paste0("q = ", q)
names(q.labs) <- q

n.labs <- paste0("n = ", n)
names(n.labs) <- n

shapex.labs <- c("X Correct", "X Incorrect")
names(shapex.labs) <- x.shape

shapec.labs <- c("C Correct", "C Incorrect")
names(shapec.labs) <- c.shape

mx.labs <- paste0("mx = ", mx)
names(mx.labs) <- mx

mc.labs <- paste0("mc = ", mc)
names(mc.labs) <- mc

my.labs <- paste0("my = ", my)
names(my.labs) <- my

param.labs <- c("\u03B2\u2080", "\u03B2\u2081", "log\u03C3\u00B2")

```

```{r}

# boxplot of simulated estimates
make.est.plot <- function(param., n. = 10000, est_cutoff = Inf) {
  
  sim.out.long %>% 
    filter(param == param.,
           n == n.,
           abs(estimate - B.true) < est_cutoff) %>%  
    ggplot(aes(y = estimate,
               fill = method)) +
    geom_boxplot() +
    geom_hline(aes(yintercept = B.true),
               linetype = "dashed",
               color = "orange") +
    facet_nested(q ~ x.shape + c.shape,
                 scales = "free",
                 labeller = labeller(q = q.labs,
                                     x.shape = shapex.labs,
                                     c.shape = shapec.labs)) +
    labs(y = "Parameter Estimate",
         fill = "Method") +
    ggtitle(paste0("Empirical Distribution of Parameter Estimates for ",
                   param.labs[param.]),
            subtitle = paste0("sample size n = ", n., "; ",
                              "mx = ", mx, "; ",
                              "mc = ", mc, "; ",
                              "my = ", my, "; ",
                              n.rep, " replicates per setting")) +
    theme_bw() +
    theme(axis.ticks.x = element_blank(),
          axis.text.x = element_blank()) +
    scale_fill_discrete(labels = method.labs)
}

```

We now look at plots of parameter estimates over the various settings. For the heavy censoring setting ($q = 0.8$), some of the ML estimates are extremely large, making the plots difficult to read.

```{r}

make.est.plot(param. = 1, n. = 10000)
make.est.plot(param. = 2, n. = 10000)
make.est.plot(param. = 3, n. = 10000)

```

Below are the same plots with the extreme ML estimates filtered out. Some observations:

* The complete case estimator has low precision under high censoring and for the parameter $\beta_2$ (the coefficient for $X$).
* For both moderate ($q=0.4$) and heavy censoring ($q=0.8$), and for both parameters, the MLE is biased when $X$ is misspecified.
* When $X$ is misspecified but $C$ is correctly specified, the semiparametric estimator is (almost) unbiased.

```{r}

make.est.plot(param. = 1, n. = 10000, est_cutoff = 0.5)
make.est.plot(param. = 2, n. = 10000, est_cutoff = 2)
make.est.plot(param. = 3, n. = 10000, est_cutoff = 0.5)

```

```{r include = F, echo = F}

tbl <- sim.out.long %>%
  group_by(q, x.shape, c.shape, method, param) %>% 
  summarise(bias = mean(estimate - B.true, na.rm = T),
            emp.se = sd(estimate, na.rm = T),
            mse = mean((estimate - B.true) ^ 2, na.rm = T)) %>% 
  gather(key, value, bias:mse) %>% 
  unite(Group, param, key) %>% 
  spread(Group, value)

setNames(tbl, sub(".+_", "", names(tbl))) %>% 
  kable(digits = 2) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 4,
                     "\u03B2\u2080" = 3,
                     "\u03B2\u2081" = 3,
                     "log\u03C3\u00B2" = 3))

```

## Next Steps

* Estimate the nuisance densities $\eta_1, \eta_2$.
* Extend the model to include a (discrete) uncensored covariate $Z$.
* Consider moving computationally intensive tasks to C++.
* Implement variance estimator.




